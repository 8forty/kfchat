healthCheckTimeout: 1200

models:
  ## deepseek ##########################################################################

  ## gemma3 ##########################################################################
  "gemma-3-1b-it-Q4_K_M.gguf":
    proxy: &kfproxy "http://127.0.0.1:27272"
    cmd: >
      c:/llama.cpp/llama-b5365-bin-win-vulkan-x64/llama-server
      -ngl 999
      -dev Vulkan1 
      -c 2048 
      -m z:/huggingface.co/converted/gemma-3-1b-it-Q4_K_M.gguf 
      --port 27272

  "gemma-3-4b-it-Q4_K_M.gguf":
    proxy: *kfproxy
    cmd: >
      c:/llama.cpp/llama-b5365-bin-win-vulkan-x64/llama-server
      -ngl 999
      -dev Vulkan1 
      -c 2048 
      -m z:/huggingface.co/converted/gemma-3-4b-it-Q4_K_M.gguf 
      --port 27272

  "gemma-3-4b-it-Q4_K_M.gguf-fa":
    proxy: *kfproxy
    cmd: >
      c:/llama.cpp/llama-b5365-bin-win-vulkan-x64/llama-server
      -ngl 999
      -dev Vulkan1 
      -c 2048 
      --flash-attn
      -m z:/huggingface.co/converted/gemma-3-4b-it-Q4_K_M.gguf 
      --port 27272

  "gemma-3-12b-it-Q4_K_M.gguf":
    proxy: *kfproxy
    cmd: >
      c:/llama.cpp/llama-b5365-bin-win-vulkan-x64/llama-server
      -ngl 999
      -dev Vulkan1 
      -c 2048 
      -m z:/huggingface.co/converted/gemma-3-12b-it-Q4_K_M.gguf 
      --port 27272

  "gemma-3-12b-it-Q4_K_M.gguf-fa":
    proxy: *kfproxy
    cmd: >
      c:/llama.cpp/llama-b5365-bin-win-vulkan-x64/llama-server
      -ngl 999
      -dev Vulkan1 
      -c 2048 
      --flash-attn
      -m z:/huggingface.co/converted/gemma-3-12b-it-Q4_K_M.gguf 
      --port 27272

  "gemma-3-27b-it-Q4_K_M.gguf":
    proxy: *kfproxy
    cmd: >
      c:/llama.cpp/llama-b5365-bin-win-vulkan-x64/llama-server
      -ngl 999
      -dev Vulkan1 
      -c 2048 
      -m z:/huggingface.co/converted/gemma-3-27b-it-Q4_K_M.gguf 
      --port 27272

  ## llama ##########################################################################

  "llama-3.2-3b-instruct-Q4_K_M.gguf":
    proxy: *kfproxy
    cmd: >
      c:/llama.cpp/llama-b5365-bin-win-vulkan-x64/llama-server
      -ngl 999
      -dev Vulkan1 
      -c 2048 
      -m z:/huggingface.co/converted/Llama-3.2-3B-Instruct-Q4_K_M.gguf 
      --port 27272

  "llama-3.3-70b-instruct-Q4_K_M.gguf":
    proxy: *kfproxy
    cmd: >
      c:/llama.cpp/llama-b5365-bin-win-vulkan-x64/llama-server
      -ngl 999
      -dev Vulkan1 
      -c 2048 
      -m z:/huggingface.co/converted/Llama-3.3-70B-Instruct-Q4_K_M.gguf 
      --port 27272

  ## phi4 ##########################################################################

  "phi-4-Q4_K_M.gguf":
    proxy: *kfproxy
    cmd: >
      c:/llama.cpp/llama-b5365-bin-win-vulkan-x64/llama-server
      -ngl 999
      -dev Vulkan1 
      -c 2048 
      -m z:/huggingface.co/converted/phi-4-Q4_K_M.gguf 
      --port 27272


  "phi-4-f16.gguf":
    proxy: *kfproxy
    cmd: >
      c:/llama.cpp/llama-b5365-bin-win-vulkan-x64/llama-server
      -ngl 999
      -dev Vulkan1 
      -c 2048 
      -m z:/huggingface.co/converted/phi-4-f16.gguf 
      --port 27272
