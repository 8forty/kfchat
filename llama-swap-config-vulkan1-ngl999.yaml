healthCheckTimeout: 12000

models:
  ## command ##########################################################################
  "c4ai-command-r7b-12-2024-Q4_K_M.gguf":
    proxy: &kfproxy "http://127.0.0.1:27272"
    cmd: >
      c:/llama.cpp/llama-b5365-bin-win-vulkan-x64/llama-server
      -ngl 999
      -dev Vulkan1 
      -c 2048 
      -m d:/huggingface.co/converted/c4ai-command-r7b-12-2024-Q4_K_M.gguf
      --port 27272
  "c4ai-command-r7b-12-2024-Q4_K_M.gguf-fa":
    proxy: &kfproxy "http://127.0.0.1:27272"
    cmd: >
      c:/llama.cpp/llama-b5365-bin-win-vulkan-x64/llama-server
      -ngl 999
      -dev Vulkan1 
      -c 2048 
      --flash-attn
      -m d:/huggingface.co/converted/c4ai-command-r7b-12-2024-Q4_K_M.gguf
      --port 27272

  ## deepseek ##########################################################################
  "deepseek-r1-distill-qwen-32b-Q4_K_M.gguf":
    proxy: &kfproxy "http://127.0.0.1:27272"
    cmd: >
      c:/llama.cpp/llama-b5365-bin-win-vulkan-x64/llama-server
      -ngl 999
      -dev Vulkan1 
      -c 2048 
      -m d:/huggingface.co/converted/deepseek-r1-distill-qwen-32b-Q4_K_M.gguf
      --port 27272
  "deepseek-r1-distill-qwen-32b-Q4_K_M.gguf-fa":
    proxy: &kfproxy "http://127.0.0.1:27272"
    cmd: >
      c:/llama.cpp/llama-b5365-bin-win-vulkan-x64/llama-server
      -ngl 999
      -dev Vulkan1 
      -c 2048 
      --flash-attn
      -m d:/huggingface.co/converted/deepseek-r1-distill-qwen-32b-Q4_K_M.gguf
      --port 27272

  ## gemma3 ##########################################################################
  "gemma-3-1b-it-Q4_K_M.gguf":
    proxy: &kfproxy "http://127.0.0.1:27272"
    cmd: >
      c:/llama.cpp/llama-b5365-bin-win-vulkan-x64/llama-server
      -ngl 999
      -dev Vulkan1 
      -c 2048 
      -m d:/huggingface.co/converted/gemma-3-1b-it-Q4_K_M.gguf 
      --port 27272
  "gemma-3-1b-it-Q4_K_M.gguf-fa":
    proxy: &kfproxy "http://127.0.0.1:27272"
    cmd: >
      c:/llama.cpp/llama-b5365-bin-win-vulkan-x64/llama-server
      -ngl 999
      -dev Vulkan1 
      -c 2048 
      --flash-attn
      -m d:/huggingface.co/converted/gemma-3-1b-it-Q4_K_M.gguf 
      --port 27272

  "gemma-3-4b-it-Q4_K_M.gguf":
    proxy: *kfproxy
    cmd: >
      c:/llama.cpp/llama-b5365-bin-win-vulkan-x64/llama-server
      -ngl 999
      -dev Vulkan1 
      -c 2048 
      -m d:/huggingface.co/converted/gemma-3-4b-it-Q4_K_M.gguf 
      --port 27272
  "gemma-3-4b-it-Q4_K_M.gguf-fa":
    proxy: *kfproxy
    cmd: >
      c:/llama.cpp/llama-b5365-bin-win-vulkan-x64/llama-server
      -ngl 999
      -dev Vulkan1 
      -c 2048 
      --flash-attn
      -m d:/huggingface.co/converted/gemma-3-4b-it-Q4_K_M.gguf 
      --port 27272

  "gemma-3-12b-it-Q4_K_M.gguf":
    proxy: *kfproxy
    cmd: >
      c:/llama.cpp/llama-b5365-bin-win-vulkan-x64/llama-server
      -ngl 999
      -dev Vulkan1 
      -c 2048 
      -m d:/huggingface.co/converted/gemma-3-12b-it-Q4_K_M.gguf 
      --port 27272
  "gemma-3-12b-it-Q4_K_M.gguf-fa":
    proxy: *kfproxy
    cmd: >
      c:/llama.cpp/llama-b5365-bin-win-vulkan-x64/llama-server
      -ngl 999
      -dev Vulkan1 
      -c 2048 
      --flash-attn
      -m d:/huggingface.co/converted/gemma-3-12b-it-Q4_K_M.gguf 
      --port 27272

  "gemma-3-27b-it-Q4_K_M.gguf":
    proxy: *kfproxy
    cmd: >
      c:/llama.cpp/llama-b5365-bin-win-vulkan-x64/llama-server
      -ngl 999
      -dev Vulkan1 
      -c 2048 
      -m d:/huggingface.co/converted/gemma-3-27b-it-Q4_K_M.gguf 
      --port 27272
  "gemma-3-27b-it-Q4_K_M.gguf-fa":
    proxy: *kfproxy
    cmd: >
      c:/llama.cpp/llama-b5365-bin-win-vulkan-x64/llama-server
      -ngl 999
      -dev Vulkan1 
      -c 2048 
      --flash-attn
      -m d:/huggingface.co/converted/gemma-3-27b-it-Q4_K_M.gguf 
      --port 27272

  ## llama ##########################################################################
  "llama-3.2-3b-instruct-Q4_K_M.gguf":
    proxy: *kfproxy
    cmd: >
      c:/llama.cpp/llama-b5365-bin-win-vulkan-x64/llama-server
      -ngl 999
      -dev Vulkan1 
      -c 2048 
      -m d:/huggingface.co/converted/Llama-3.2-3B-Instruct-Q4_K_M.gguf 
      --port 27272
  "llama-3.2-3b-instruct-Q4_K_M.gguf-fa":
    proxy: *kfproxy
    cmd: >
      c:/llama.cpp/llama-b5365-bin-win-vulkan-x64/llama-server
      -ngl 999
      -dev Vulkan1 
      -c 2048 
      --flash-attn
      -m d:/huggingface.co/converted/Llama-3.2-3B-Instruct-Q4_K_M.gguf 
      --port 27272

  "llama-3.3-70b-instruct-Q4_K_M.gguf":
    proxy: *kfproxy
    cmd: >
      c:/llama.cpp/llama-b5365-bin-win-vulkan-x64/llama-server
      -ngl 999
      -dev Vulkan1 
      -c 2048 
      -m d:/huggingface.co/converted/Llama-3.3-70B-Instruct-Q4_K_M.gguf 
      --port 27272
  "llama-3.3-70b-instruct-Q4_K_M.gguf-fa":
    proxy: *kfproxy
    # ngl
    cmd: >
      c:/llama.cpp/llama-b5365-bin-win-vulkan-x64/llama-server
      -ngl 999
      -dev Vulkan1 
      -c 2048 
      --flash-attn
      -m d:/huggingface.co/converted/Llama-3.3-70B-Instruct-Q4_K_M.gguf 
      --port 27272

  ## mistral ##########################################################################
  "mistral-nemo-instruct-2407-Q4_K_M.gguf":
    proxy: *kfproxy
    cmd: >
      c:/llama.cpp/llama-b5365-bin-win-vulkan-x64/llama-server
      -ngl 999
      -dev Vulkan1 
      -c 2048 
      -m d:/huggingface.co/converted/mistral-nemo-instruct-2407-Q4_K_M.gguf
      --port 27272
  "mistral-nemo-instruct-2407-Q4_K_M.gguf-fa":
    proxy: *kfproxy
    cmd: >
      c:/llama.cpp/llama-b5365-bin-win-vulkan-x64/llama-server
      -ngl 999
      -dev Vulkan1 
      -c 2048 
      --flash-attn
      -m d:/huggingface.co/converted/mistral-nemo-instruct-2407-Q4_K_M.gguf
      --port 27272

  ## phi4 ##########################################################################
  "phi-4-Q4_K_M.gguf":
    proxy: *kfproxy
    cmd: >
      c:/llama.cpp/llama-b5365-bin-win-vulkan-x64/llama-server
      -ngl 999
      -dev Vulkan1 
      -c 2048 
      -m d:/huggingface.co/converted/phi-4-Q4_K_M.gguf 
      --port 27272
  "phi-4-Q4_K_M.gguf-fa":
    proxy: *kfproxy
    cmd: >
      c:/llama.cpp/llama-b5365-bin-win-vulkan-x64/llama-server
      -ngl 999
      -dev Vulkan1 
      -c 2048 
      --flash-attn
      -m d:/huggingface.co/converted/phi-4-Q4_K_M.gguf 
      --port 27272

  "phi-4-f16.gguf":
    proxy: *kfproxy
    cmd: >
      c:/llama.cpp/llama-b5365-bin-win-vulkan-x64/llama-server
      -ngl 999
      -dev Vulkan1 
      -c 2048 
      -m d:/huggingface.co/converted/phi-4-f16.gguf 
      --port 27272
  "phi-4-f16.gguf-fa":
    proxy: *kfproxy
    # ngl
    cmd: >
      c:/llama.cpp/llama-b5365-bin-win-vulkan-x64/llama-server
      -ngl 999
      -dev Vulkan1 
      -c 2048 
      --flash-attn
      -m d:/huggingface.co/converted/phi-4-f16.gguf 
      --port 27272

  ## qwen3 ##########################################################################
  "qwen3-14b-Q4_K_M.gguf":
    proxy: *kfproxy
    cmd: >
      c:/llama.cpp/llama-b5365-bin-win-vulkan-x64/llama-server
      -ngl 999
      -dev Vulkan1 
      -c 2048 
      -m d:/huggingface.co/converted/qwen3-14b-Q4_K_M.gguf
      --port 27272
  "qwen3-14b-Q4_K_M.gguf-fa":
    proxy: *kfproxy
    cmd: >
      c:/llama.cpp/llama-b5365-bin-win-vulkan-x64/llama-server
      -ngl 999
      -dev Vulkan1 
      -c 2048 
      --flash-attn
      -m d:/huggingface.co/converted/qwen3-14b-Q4_K_M.gguf
      --port 27272

  "qwen3-32b-Q4_K_M.gguf":
    proxy: *kfproxy
    cmd: >
      c:/llama.cpp/llama-b5365-bin-win-vulkan-x64/llama-server
      -ngl 999
      -dev Vulkan1 
      -c 2048 
      -m d:/huggingface.co/converted/qwen3-32b-Q4_K_M.gguf
      --port 27272
  "qwen3-32b-Q4_K_M.gguf-fa":
    proxy: *kfproxy
    cmd: >
      c:/llama.cpp/llama-b5365-bin-win-vulkan-x64/llama-server
      -ngl 999
      -dev Vulkan1 
      -c 2048 
      --flash-attn
      -m d:/huggingface.co/converted/qwen3-32b-Q4_K_M.gguf
      --port 27272
